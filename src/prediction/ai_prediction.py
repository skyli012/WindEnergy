import streamlit as st
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.svm import SVR
import plotly.graph_objects as go
import plotly.express as px
import plotly.subplots as sp
import time
import scipy.stats as stats
import warnings

warnings.filterwarnings('ignore')

# æ·±åº¦å­¦ä¹ åº“
try:
    import tensorflow as tf
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import LSTM, GRU, Dense, Dropout, Input
    from tensorflow.keras.optimizers import Adam
    from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau

    HAS_TENSORFLOW = True
except Exception:
    HAS_TENSORFLOW = False

# å¯é€‰åº“
try:
    from xgboost import XGBRegressor

    HAS_XGBOOST = True
except Exception:
    HAS_XGBOOST = False

try:
    from lightgbm import LGBMRegressor

    HAS_LIGHTGBM = True
except Exception:
    HAS_LIGHTGBM = False

try:
    from catboost import CatBoostRegressor

    HAS_CATBOOST = True
except Exception:
    HAS_CATBOOST = False


# ===================== æ·±åº¦å­¦ä¹ æ¨¡å‹æ„å»ºå‡½æ•° =====================
def create_lstm_model(input_shape, units=50, dropout_rate=0.2, learning_rate=0.001):
    """åˆ›å»ºLSTMæ¨¡å‹"""
    model = Sequential([
        Input(shape=input_shape),
        LSTM(units, return_sequences=True, dropout=dropout_rate),
        LSTM(units // 2, dropout=dropout_rate),
        Dense(32, activation='relu'),
        Dropout(dropout_rate),
        Dense(16, activation='relu'),
        Dense(1)
    ])

    model.compile(
        optimizer=Adam(learning_rate=learning_rate),
        loss='mse',
        metrics=['mae']
    )
    return model


def create_gru_model(input_shape, units=50, dropout_rate=0.2, learning_rate=0.001):
    """åˆ›å»ºGRUæ¨¡å‹"""
    model = Sequential([
        Input(shape=input_shape),
        GRU(units, return_sequences=True, dropout=dropout_rate),
        GRU(units // 2, dropout=dropout_rate),
        Dense(32, activation='relu'),
        Dropout(dropout_rate),
        Dense(16, activation='relu'),
        Dense(1)
    ])

    model.compile(
        optimizer=Adam(learning_rate=learning_rate),
        loss='mse',
        metrics=['mae']
    )
    return model


def prepare_sequences_for_dl(X, y, time_steps=10):
    """ä¸ºæ·±åº¦å­¦ä¹ æ¨¡å‹å‡†å¤‡æ—¶é—´åºåˆ—æ•°æ®"""
    X_sequences = []
    y_sequences = []

    for i in range(time_steps, len(X)):
        X_sequences.append(X[i - time_steps:i])
        y_sequences.append(y[i])

    return np.array(X_sequences), np.array(y_sequences)


# ===================== ä¸»é¡µé¢ =====================
def ai_prediction_page():
    st.title("ğŸ¤– Szeged é£é€ŸAIé¢„æµ‹åˆ†æç³»ç»Ÿ")

    # æ•°æ®çŠ¶æ€æ£€æŸ¥
    if 'dataset' not in st.session_state:
        st.warning("âš ï¸ è¯·å…ˆåœ¨æ•°æ®å¯¼å…¥é¡µé¢å¯¼å…¥æ°”è±¡æ•°æ®")
        return

    df = st.session_state['dataset'].copy()

    # ä¾§è¾¹æ é…ç½®
    with st.sidebar:
        st.header("âš™ï¸ åˆ†æé…ç½®")
        analysis_mode = st.radio(
            "åˆ†ææ¨¡å¼",
            ["å•æ¨¡å‹é¢„æµ‹", "å¤šæ¨¡å‹å¯¹æ¯”", "æ·±åº¦åˆ†æ"]
        )

        if analysis_mode == "æ·±åº¦åˆ†æ":
            st.info("ğŸ” æ·±åº¦åˆ†ææ¨¡å¼å°†æä¾›æ›´è¯¦ç»†çš„ç‰¹å¾é‡è¦æ€§ã€è¯¯å·®åˆ†æå’Œé¢„æµ‹è§£é‡Š")

    # æ—¶é—´ç‰¹å¾å¤„ç†
    datetime_col = next(
        (col for col in df.columns if 'time' in col.lower() or 'datatime' in col.lower() or 'date' in col.lower()),
        None)
    if datetime_col:
        df[datetime_col] = pd.to_datetime(df[datetime_col], errors='coerce')
        df['hour'] = df[datetime_col].dt.hour
        df['month'] = df[datetime_col].dt.month
        df['dayofyear'] = df[datetime_col].dt.dayofyear
        df['dayofweek'] = df[datetime_col].dt.dayofweek
        df['is_weekend'] = df['dayofweek'].isin([5, 6]).astype(int)
        df['season'] = (df[datetime_col].dt.month % 12 + 3) // 3  # å­£èŠ‚åˆ’åˆ†
    else:
        st.error("âŒ æœªæ‰¾åˆ°æ—¶é—´åˆ—")
        return

    # ç›®æ ‡å˜é‡
    target_candidates = ['wind_speed_ms']
    target_column = next((col for col in target_candidates if col in df.columns), None)
    if not target_column:
        st.error("âŒ æœªæ‰¾åˆ°ç›®æ ‡å˜é‡")
        return

    # æ•°æ®æ¦‚è§ˆ
    st.subheader("ğŸ“Š æ•°æ®æ¦‚è§ˆ")
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("æ€»æ ·æœ¬æ•°", f"{len(df):,}")
    with col2:
        st.metric("å¹³å‡é£é€Ÿ", f"{df[target_column].mean():.2f} m/s")
    with col3:
        st.metric("é£é€Ÿæ ‡å‡†å·®", f"{df[target_column].std():.2f} m/s")
    with col4:
        st.metric("æ•°æ®æ—¶é—´èŒƒå›´",
                  f"{df[datetime_col].min().strftime('%Y-%m')} è‡³ {df[datetime_col].max().strftime('%Y-%m')}")

    if analysis_mode == "å•æ¨¡å‹é¢„æµ‹":
        single_model_analysis(df, datetime_col, target_column)
    elif analysis_mode == "å¤šæ¨¡å‹å¯¹æ¯”":
        multi_model_comparison(df, datetime_col, target_column)
    elif analysis_mode == "æ·±åº¦åˆ†æ":
        deep_analysis(df, datetime_col, target_column)


# ===================== å•æ¨¡å‹åˆ†æ =====================
def single_model_analysis(df, datetime_col, target_column):
    st.subheader("ğŸ¯ å•æ¨¡å‹é¢„æµ‹åˆ†æ")

    # ç‰¹å¾é€‰æ‹©
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    exclude_cols = [target_column]
    feature_candidates = [c for c in numeric_cols if c not in exclude_cols]

    col1, col2 = st.columns([2, 1])

    with col1:
        selected_features = st.multiselect(
            "é€‰æ‹©ç‰¹å¾å˜é‡",
            options=feature_candidates,
            default=[col for col in [
                'temperature_c', 'humidity', 'pressure_millibars', 'wind_bearing_degrees',
                'visibility_ms', 'hour', 'month', 'dayofyear', 'season'
            ] if col in feature_candidates]
        )

    with col2:
        # æ¨¡å‹é€‰æ‹© - æ·»åŠ æ·±åº¦å­¦ä¹ æ¨¡å‹
        model_options = ["éšæœºæ£®æ—", "æ¢¯åº¦æå‡", "XGBoost", "LightGBM", "CatBoost",
                         "çº¿æ€§å›å½’", "æ”¯æŒå‘é‡æœº", "LSTM", "GRU"]

        # æ£€æŸ¥åº“å¯ç”¨æ€§
        available_models = []
        for model in model_options:
            if model in ["XGBoost", "LightGBM", "CatBoost"]:
                if globals().get(f'HAS_{model.upper()}'):
                    available_models.append(model)
            elif model in ["LSTM", "GRU"]:
                if HAS_TENSORFLOW:
                    available_models.append(model)
            else:
                available_models.append(model)

        model_option = st.selectbox("é€‰æ‹©ç®—æ³•", available_models)

        # é«˜çº§å‚æ•°
        with st.expander("é«˜çº§å‚æ•°"):
            test_size = st.slider("æµ‹è¯•é›†æ¯”ä¾‹", 0.1, 0.4, 0.2, 0.05)
            cv_folds = st.slider("äº¤å‰éªŒè¯æŠ˜æ•°", 3, 10, 5)
            enable_permutation = st.checkbox("å¯ç”¨ç½®æ¢é‡è¦æ€§åˆ†æ", value=True)

            # æ·±åº¦å­¦ä¹ ç‰¹å®šå‚æ•°
            if model_option in ["LSTM", "GRU"]:
                time_steps = st.slider("æ—¶é—´æ­¥é•¿", 5, 50, 10,
                                       help="è€ƒè™‘çš„å†å²æ—¶é—´æ­¥æ•°")
                lstm_units = st.slider("LSTM/GRUå•å…ƒæ•°", 16, 128, 50)
                epochs = st.slider("è®­ç»ƒè½®æ•°", 10, 200, 50)
                batch_size = st.slider("æ‰¹å¤§å°", 16, 128, 32)
                learning_rate = st.slider("å­¦ä¹ ç‡", 0.0001, 0.01, 0.001, 0.0001)

    if not selected_features:
        st.warning("è¯·é€‰æ‹©è‡³å°‘ä¸€ä¸ªç‰¹å¾å˜é‡")
        return

    if st.button("ğŸš€ å¼€å§‹è®­ç»ƒåˆ†æ", type="primary", use_container_width=True):
        with st.spinner("æ­£åœ¨è¿›è¡Œæ¨¡å‹è®­ç»ƒå’Œæ·±åº¦åˆ†æ..."):
            # æ•°æ®å‡†å¤‡
            X = df[selected_features].fillna(0)
            y = df[target_column].fillna(0)

            # æ•°æ®åˆ†å‰²
            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)

            scaler = StandardScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            X_test_scaled = scaler.transform(X_test)

            # æ¨¡å‹è®­ç»ƒ
            model = None
            training_time = 0
            history = None

            if model_option == "éšæœºæ£®æ—":
                model = RandomForestRegressor(n_estimators=200, random_state=42, n_jobs=-1)
            elif model_option == "æ¢¯åº¦æå‡":
                model = GradientBoostingRegressor(n_estimators=200, learning_rate=0.05, random_state=42)
            elif model_option == "XGBoost":
                model = XGBRegressor(n_estimators=200, random_state=42, verbosity=0, n_jobs=-1)
            elif model_option == "LightGBM":
                model = LGBMRegressor(n_estimators=200, learning_rate=0.05, n_jobs=-1, random_state=42)
            elif model_option == "CatBoost":
                model = CatBoostRegressor(iterations=200, learning_rate=0.05, verbose=0, random_state=42)
            elif model_option == "çº¿æ€§å›å½’":
                model = LinearRegression()
            elif model_option == "æ”¯æŒå‘é‡æœº":
                model = SVR(kernel='rbf', C=1.0, epsilon=0.1)
            elif model_option == "LSTM":
                # å‡†å¤‡æ—¶é—´åºåˆ—æ•°æ®
                X_train_seq, y_train_seq = prepare_sequences_for_dl(X_train_scaled, y_train.values, time_steps)
                X_test_seq, y_test_seq = prepare_sequences_for_dl(X_test_scaled, y_test.values, time_steps)

                # åˆ›å»ºæ¨¡å‹
                model = create_lstm_model(
                    input_shape=(time_steps, len(selected_features)),
                    units=lstm_units,
                    learning_rate=learning_rate
                )

                # è®­ç»ƒæ¨¡å‹
                start_time = time.time()
                history = model.fit(
                    X_train_seq, y_train_seq,
                    epochs=epochs,
                    batch_size=batch_size,
                    validation_data=(X_test_seq, y_test_seq),
                    verbose=0,
                    callbacks=[
                        EarlyStopping(patience=10, restore_best_weights=True),
                        ReduceLROnPlateau(patience=5, factor=0.5)
                    ]
                )
                training_time = time.time() - start_time

                # é¢„æµ‹
                y_pred = model.predict(X_test_seq).flatten()
                y_test = y_test_seq

            elif model_option == "GRU":
                # å‡†å¤‡æ—¶é—´åºåˆ—æ•°æ®
                X_train_seq, y_train_seq = prepare_sequences_for_dl(X_train_scaled, y_train.values, time_steps)
                X_test_seq, y_test_seq = prepare_sequences_for_dl(X_test_scaled, y_test.values, time_steps)

                # åˆ›å»ºæ¨¡å‹
                model = create_gru_model(
                    input_shape=(time_steps, len(selected_features)),
                    units=lstm_units,
                    learning_rate=learning_rate
                )

                # è®­ç»ƒæ¨¡å‹
                start_time = time.time()
                history = model.fit(
                    X_train_seq, y_train_seq,
                    epochs=epochs,
                    batch_size=batch_size,
                    validation_data=(X_test_seq, y_test_seq),
                    verbose=0,
                    callbacks=[
                        EarlyStopping(patience=10, restore_best_weights=True),
                        ReduceLROnPlateau(patience=5, factor=0.5)
                    ]
                )
                training_time = time.time() - start_time

                # é¢„æµ‹
                y_pred = model.predict(X_test_seq).flatten()
                y_test = y_test_seq

            # ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹çš„è®­ç»ƒå’Œé¢„æµ‹
            if model_option not in ["LSTM", "GRU"]:
                start_time = time.time()
                model.fit(X_train_scaled, y_train)
                training_time = time.time() - start_time
                y_pred = model.predict(X_test_scaled)

            # äº¤å‰éªŒè¯ï¼ˆä»…å¯¹ä¼ ç»Ÿæ¨¡å‹ï¼‰
            cv_scores = []
            if model_option not in ["LSTM", "GRU"]:
                cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=cv_folds, scoring='r2')

            # è®¡ç®—æŒ‡æ ‡
            results = calculate_metrics(y_test, y_pred, training_time)
            if model_option not in ["LSTM", "GRU"]:
                results['cv_mean'] = cv_scores.mean() if len(cv_scores) > 0 else 0
                results['cv_std'] = cv_scores.std() if len(cv_scores) > 0 else 0
            else:
                results['cv_mean'] = 0
                results['cv_std'] = 0

            # ç‰¹å¾é‡è¦æ€§ï¼ˆä»…å¯¹æ”¯æŒç‰¹å¾é‡è¦æ€§çš„æ¨¡å‹ï¼‰
            feature_importance = None
            permutation_importance_result = None

            if hasattr(model, 'feature_importances_'):
                feature_importance = pd.DataFrame({
                    'feature': selected_features,
                    'importance': model.feature_importances_
                }).sort_values('importance', ascending=False)

            # ç½®æ¢é‡è¦æ€§
            if enable_permutation and model_option not in ["LSTM", "GRU"]:
                with st.spinner("è®¡ç®—ç½®æ¢é‡è¦æ€§ä¸­..."):
                    permutation_importance_result = calculate_permutation_importance(
                        model, X_test_scaled, y_test, selected_features
                    )

            # æ˜¾ç¤ºç»“æœ
            display_single_model_results(
                results, feature_importance, permutation_importance_result,
                model_option, y_test, y_pred, cv_scores, X_test_scaled, model, history
            )


# ===================== å¤šæ¨¡å‹å¯¹æ¯” =====================
def multi_model_comparison(df, datetime_col, target_column):
    st.subheader("âš–ï¸ å¤šæ¨¡å‹å¯¹æ¯”åˆ†æ")

    # ç‰¹å¾é€‰æ‹©
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    exclude_cols = [target_column]
    feature_candidates = [c for c in numeric_cols if c not in exclude_cols]

    selected_features = st.multiselect(
        "é€‰æ‹©ç‰¹å¾å˜é‡",
        options=feature_candidates,
        default=[col for col in [
            'temperature_c', 'humidity', 'pressure_millibars', 'wind_bearing_degrees',
            'visibility_ms', 'hour', 'month', 'dayofyear', 'season'
        ] if col in feature_candidates]
    )

    # æ¨¡å‹é€‰æ‹© - æ·»åŠ æ·±åº¦å­¦ä¹ æ¨¡å‹
    model_options = ["éšæœºæ£®æ—", "æ¢¯åº¦æå‡", "XGBoost", "LightGBM", "CatBoost",
                     "çº¿æ€§å›å½’", "æ”¯æŒå‘é‡æœº", "LSTM", "GRU"]

    # æ£€æŸ¥åº“å¯ç”¨æ€§
    available_models = []
    for model in model_options:
        if model in ["XGBoost", "LightGBM", "CatBoost"]:
            if globals().get(f'HAS_{model.upper()}'):
                available_models.append(model)
        elif model in ["LSTM", "GRU"]:
            if HAS_TENSORFLOW:
                available_models.append(model)
        else:
            available_models.append(model)

    selected_algorithms = st.multiselect(
        "é€‰æ‹©å¯¹æ¯”ç®—æ³•",
        options=available_models,
        default=available_models[:4]  # é»˜è®¤é€‰æ‹©å‰4ä¸ªå¯ç”¨æ¨¡å‹
    )

    # æ·±åº¦å­¦ä¹ å‚æ•°
    dl_params = {}
    if any(model in selected_algorithms for model in ["LSTM", "GRU"]):
        with st.expander("æ·±åº¦å­¦ä¹ å‚æ•°é…ç½®"):
            time_steps = st.slider("æ—¶é—´æ­¥é•¿", 5, 50, 10)
            lstm_units = st.slider("LSTM/GRUå•å…ƒæ•°", 16, 128, 50)
            epochs = st.slider("è®­ç»ƒè½®æ•°", 10, 100, 30)
            batch_size = st.slider("æ‰¹å¤§å°", 16, 128, 32)
            dl_params = {
                'time_steps': time_steps,
                'units': lstm_units,
                'epochs': epochs,
                'batch_size': batch_size
            }

    if st.button("ğŸ”¬ å¼€å§‹å¯¹æ¯”åˆ†æ", type="primary", use_container_width=True):
        if not selected_features or not selected_algorithms:
            st.warning("è¯·é€‰æ‹©ç‰¹å¾å˜é‡å’Œå¯¹æ¯”ç®—æ³•")
            return

        with st.spinner("æ­£åœ¨è¿›è¡Œå¤šæ¨¡å‹å¯¹æ¯”åˆ†æ..."):
            # æ•°æ®å‡†å¤‡
            X = df[selected_features].fillna(0)
            y = df[target_column].fillna(0)

            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

            scaler = StandardScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            X_test_scaled = scaler.transform(X_test)

            # è®­ç»ƒæ‰€æœ‰æ¨¡å‹
            comparison_results = []
            feature_importances = {}
            predictions = {}
            models = {}
            training_histories = {}

            progress_bar = st.progress(0)
            status_text = st.empty()

            for i, algo in enumerate(selected_algorithms):
                status_text.text(f"è®­ç»ƒ {algo}... ({i + 1}/{len(selected_algorithms)})")

                try:
                    model = None
                    history = None

                    if algo == "éšæœºæ£®æ—":
                        model = RandomForestRegressor(n_estimators=200, random_state=42, n_jobs=-1)
                    elif algo == "æ¢¯åº¦æå‡":
                        model = GradientBoostingRegressor(n_estimators=200, learning_rate=0.05, random_state=42)
                    elif algo == "XGBoost":
                        model = XGBRegressor(n_estimators=200, random_state=42, verbosity=0, n_jobs=-1)
                    elif algo == "LightGBM":
                        model = LGBMRegressor(n_estimators=200, learning_rate=0.05, n_jobs=-1, random_state=42)
                    elif algo == "CatBoost":
                        model = CatBoostRegressor(iterations=200, learning_rate=0.05, verbose=0, random_state=42)
                    elif algo == "çº¿æ€§å›å½’":
                        model = LinearRegression()
                    elif algo == "æ”¯æŒå‘é‡æœº":
                        model = SVR(kernel='rbf', C=1.0, epsilon=0.1)
                    elif algo == "LSTM":
                        # å‡†å¤‡æ—¶é—´åºåˆ—æ•°æ®
                        X_train_seq, y_train_seq = prepare_sequences_for_dl(
                            X_train_scaled, y_train.values, dl_params['time_steps'])
                        X_test_seq, y_test_seq = prepare_sequences_for_dl(
                            X_test_scaled, y_test.values, dl_params['time_steps'])

                        model = create_lstm_model(
                            input_shape=(dl_params['time_steps'], len(selected_features)),
                            units=dl_params['units']
                        )

                        start_time = time.time()
                        history = model.fit(
                            X_train_seq, y_train_seq,
                            epochs=dl_params['epochs'],
                            batch_size=dl_params['batch_size'],
                            validation_data=(X_test_seq, y_test_seq),
                            verbose=0
                        )
                        training_time = time.time() - start_time

                        y_pred = model.predict(X_test_seq).flatten()
                        y_test_used = y_test_seq

                    elif algo == "GRU":
                        # å‡†å¤‡æ—¶é—´åºåˆ—æ•°æ®
                        X_train_seq, y_train_seq = prepare_sequences_for_dl(
                            X_train_scaled, y_train.values, dl_params['time_steps'])
                        X_test_seq, y_test_seq = prepare_sequences_for_dl(
                            X_test_scaled, y_test.values, dl_params['time_steps'])

                        model = create_gru_model(
                            input_shape=(dl_params['time_steps'], len(selected_features)),
                            units=dl_params['units']
                        )

                        start_time = time.time()
                        history = model.fit(
                            X_train_seq, y_train_seq,
                            epochs=dl_params['epochs'],
                            batch_size=dl_params['batch_size'],
                            validation_data=(X_test_seq, y_test_seq),
                            verbose=0
                        )
                        training_time = time.time() - start_time

                        y_pred = model.predict(X_test_seq).flatten()
                        y_test_used = y_test_seq

                    # ä¼ ç»Ÿæ¨¡å‹çš„è®­ç»ƒ
                    if algo not in ["LSTM", "GRU"]:
                        start_time = time.time()
                        model.fit(X_train_scaled, y_train)
                        training_time = time.time() - start_time
                        y_pred = model.predict(X_test_scaled)
                        y_test_used = y_test

                    predictions[algo] = y_pred
                    models[algo] = model
                    if history:
                        training_histories[algo] = history

                    # è®¡ç®—æŒ‡æ ‡
                    results = calculate_metrics(y_test_used, y_pred, training_time)

                    # ç‰¹å¾é‡è¦æ€§
                    if hasattr(model, 'feature_importances_'):
                        feature_importances[algo] = pd.DataFrame({
                            'feature': selected_features,
                            'importance': model.feature_importances_
                        }).sort_values('importance', ascending=False)

                    comparison_results.append({
                        "ç®—æ³•": algo,
                        "MAE": results['mae'],
                        "RMSE": results['rmse'],
                        "RÂ²": results['r2'],
                        "è®­ç»ƒæ—¶é—´(ç§’)": results['training_time']
                    })

                except Exception as e:
                    st.error(f"è®­ç»ƒ {algo} æ—¶å‡ºé”™: {str(e)}")
                    continue

                progress_bar.progress((i + 1) / len(selected_algorithms))

            progress_bar.empty()
            status_text.empty()

            # æ˜¾ç¤ºå¯¹æ¯”ç»“æœ
            display_comparison_results(
                comparison_results, feature_importances, y_test,
                predictions, selected_features, models, X_test_scaled, training_histories
            )


def display_comparison_results(comparison_results, feature_importances, y_true, predictions, selected_features, models,
                               X_test_scaled, training_histories=None):
    """æ˜¾ç¤ºå¤šæ¨¡å‹å¯¹æ¯”ç»“æœ"""
    st.subheader("ğŸ“‹ æ€§èƒ½å¯¹æ¯”è¡¨")
    df_comparison = pd.DataFrame(comparison_results)
    st.dataframe(df_comparison.style.format({
        "MAE": "{:.3f}", "RMSE": "{:.3f}", "RÂ²": "{:.4f}", "è®­ç»ƒæ—¶é—´(ç§’)": "{:.2f}"
    }), use_container_width=True)

    # æ·±åº¦å­¦ä¹ è®­ç»ƒå†å²å¯è§†åŒ–
    if training_histories:
        st.subheader("ğŸ“ˆ æ·±åº¦å­¦ä¹ æ¨¡å‹è®­ç»ƒè¿‡ç¨‹")
        dl_algorithms = [algo for algo in training_histories.keys() if algo in ["LSTM", "GRU"]]

        if dl_algorithms:
            fig_history = go.Figure()
            for algo in dl_algorithms:
                history = training_histories[algo]
                fig_history.add_trace(go.Scatter(
                    y=history.history['loss'],
                    mode='lines',
                    name=f'{algo} - è®­ç»ƒæŸå¤±'
                ))
                fig_history.add_trace(go.Scatter(
                    y=history.history['val_loss'],
                    mode='lines',
                    name=f'{algo} - éªŒè¯æŸå¤±',
                    line=dict(dash='dash')
                ))

            fig_history.update_layout(
                title="æ·±åº¦å­¦ä¹ æ¨¡å‹è®­ç»ƒæŸå¤±æ›²çº¿",
                xaxis_title="è®­ç»ƒè½®æ•°",
                yaxis_title="æŸå¤±å€¼",
                height=400
            )
            st.plotly_chart(fig_history, use_container_width=True)

    # ç‰¹å¾é‡è¦æ€§å¯¹æ¯”
    if feature_importances:
        st.subheader("ğŸ¯ ç‰¹å¾é‡è¦æ€§å¯¹æ¯”")
        algorithms = list(feature_importances.keys())

        # é€‰æ‹©å‰5ä¸ªç‰¹å¾è¿›è¡Œå¯¹æ¯”
        top_features = set()
        for algo in algorithms:
            top_features.update(feature_importances[algo].head(5)['feature'].tolist())
        top_features = list(top_features)[:8]  # æœ€å¤šæ˜¾ç¤º8ä¸ªç‰¹å¾

        fig = go.Figure()
        for algo in algorithms:
            algo_importance = []
            for feature in top_features:
                feature_row = feature_importances[algo][feature_importances[algo]['feature'] == feature]
                if len(feature_row) > 0:
                    algo_importance.append(feature_row['importance'].values[0])
                else:
                    algo_importance.append(0)

            fig.add_trace(go.Bar(
                name=algo,
                x=top_features,
                y=algo_importance,
                text=[f'{x:.3f}' for x in algo_importance],
                textposition='auto'
            ))

        fig.update_layout(
            barmode='group',
            title="Top ç‰¹å¾é‡è¦æ€§å¯¹æ¯”",
            xaxis_title="ç‰¹å¾",
            yaxis_title="é‡è¦æ€§åˆ†æ•°",
            height=500
        )
        st.plotly_chart(fig, use_container_width=True)

    # ================== é¢„æµ‹ vs çœŸå®å€¼æ•£ç‚¹å›¾å¯¹æ¯” ==================
    st.markdown("### ğŸ“Š é¢„æµ‹æ•ˆæœå¯¹æ¯”å›¾")

    # åˆ›å»ºæ•£ç‚¹å›¾
    fig_scatter = go.Figure()

    # é¢œè‰²åˆ—è¡¨
    colors = px.colors.qualitative.Set3

    # ä¸ºæ¯ä¸ªç®—æ³•æ·»åŠ æ•£ç‚¹
    for i, (algo, y_pred) in enumerate(predictions.items()):
        # ä¸‹é‡‡æ ·ä»¥é¿å…è¿‡åº¦æ‹¥æŒ¤
        sample_size = min(1000, len(y_true))
        if len(y_true) > sample_size:
            try:
                indices = np.random.choice(len(y_true), size=sample_size, replace=False)
                if hasattr(y_true, 'iloc'):
                    y_true_sample = y_true.iloc[indices]
                else:
                    y_true_sample = y_true[indices]
                y_pred_sample = y_pred[indices]
            except Exception as e:
                st.warning(f"é‡‡æ ·å¤±è´¥: {str(e)}ï¼Œä½¿ç”¨å…¨éƒ¨æ•°æ®")
                y_true_sample = y_true
                y_pred_sample = y_pred
        else:
            y_true_sample = y_true
            y_pred_sample = y_pred

        # è®¡ç®—è¯¥ç®—æ³•çš„RÂ²
        algo_r2 = r2_score(y_true_sample, y_pred_sample)

        fig_scatter.add_trace(go.Scatter(
            x=y_true_sample,
            y=y_pred_sample,
            mode='markers',
            name=f'{algo} (RÂ²={algo_r2:.3f})',
            marker=dict(
                color=colors[i % len(colors)],
                opacity=0.6,
                size=6
            ),
            hovertemplate='<b>çœŸå®å€¼</b>: %{x:.2f}<br><b>é¢„æµ‹å€¼</b>: %{y:.2f}<br><b>ç®—æ³•</b>: ' + algo + '<extra></extra>'
        ))

    # æ·»åŠ ç†æƒ³æ‹Ÿåˆçº¿
    min_val = min(min(y_true), min([min(pred) for pred in predictions.values()]))
    max_val = max(max(y_true), max([max(pred) for pred in predictions.values()]))
    fig_scatter.add_trace(go.Scatter(
        x=[min_val, max_val],
        y=[min_val, max_val],
        mode='lines',
        name='ç†æƒ³æ‹Ÿåˆçº¿ (y=x)',
        line=dict(dash='dash', color='black', width=2),
        hovertemplate='ç†æƒ³æ‹Ÿåˆçº¿<extra></extra>'
    ))

    fig_scatter.update_layout(
        title="é¢„æµ‹å€¼ vs çœŸå®å€¼æ•£ç‚¹å›¾å¯¹æ¯”",
        xaxis_title="çœŸå®é£é€Ÿ (m/s)",
        yaxis_title="é¢„æµ‹é£é€Ÿ (m/s)",
        legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1),
        height=600
    )
    st.plotly_chart(fig_scatter, use_container_width=True)

    # ================== æ€§èƒ½æŒ‡æ ‡æŸ±çŠ¶å›¾å¯¹æ¯” ==================
    st.markdown("### ğŸ“ˆ æ€§èƒ½æŒ‡æ ‡å¯è§†åŒ–å¯¹æ¯”")

    tab1, tab2, tab3 = st.tabs(["MAE & RMSE", "RÂ² å†³å®šç³»æ•°", "è®­ç»ƒæ—¶é—´"])

    with tab1:
        fig_metrics = go.Figure()
        algorithms = [result["ç®—æ³•"] for result in comparison_results]
        mae_values = [result["MAE"] for result in comparison_results]
        rmse_values = [result["RMSE"] for result in comparison_results]

        fig_metrics.add_trace(go.Bar(
            name='MAE',
            x=algorithms,
            y=mae_values,
            marker_color='#FF6B6B',
            text=[f'{x:.3f}' for x in mae_values],
            textposition='auto'
        ))
        fig_metrics.add_trace(go.Bar(
            name='RMSE',
            x=algorithms,
            y=rmse_values,
            marker_color='#4ECDC4',
            text=[f'{x:.3f}' for x in rmse_values],
            textposition='auto'
        ))

        fig_metrics.update_layout(
            title="MAE å’Œ RMSE å¯¹æ¯”ï¼ˆå€¼è¶Šå°è¶Šå¥½ï¼‰",
            barmode='group',
            xaxis_title="ç®—æ³•",
            yaxis_title="è¯¯å·®å€¼"
        )
        st.plotly_chart(fig_metrics, use_container_width=True)

    with tab2:
        fig_r2 = go.Figure()
        r2_values = [result["RÂ²"] for result in comparison_results]

        # æ ¹æ®RÂ²å€¼è®¾ç½®é¢œè‰²ï¼ˆè¶Šé«˜è¶Šå¥½ï¼‰
        colors_r2 = ['#FF6B6B' if x < 0.5 else '#4ECDC4' if x < 0.8 else '#1A936F' for x in r2_values]

        fig_r2.add_trace(go.Bar(
            x=algorithms,
            y=r2_values,
            marker_color=colors_r2,
            text=[f'{x:.4f}' for x in r2_values],
            textposition='auto'
        ))

        fig_r2.update_layout(
            title="RÂ² å†³å®šç³»æ•°å¯¹æ¯”ï¼ˆå€¼è¶Šæ¥è¿‘1è¶Šå¥½ï¼‰",
            xaxis_title="ç®—æ³•",
            yaxis_title="RÂ² å€¼",
            yaxis_range=[0, 1]
        )
        # æ·»åŠ å‚è€ƒçº¿
        fig_r2.add_hline(y=0.5, line_dash="dash", line_color="orange", annotation_text="ä¸€èˆ¬æ°´å¹³")
        fig_r2.add_hline(y=0.8, line_dash="dash", line_color="green", annotation_text="ä¼˜ç§€æ°´å¹³")

        st.plotly_chart(fig_r2, use_container_width=True)

    with tab3:
        fig_time = go.Figure()
        time_values = [result["è®­ç»ƒæ—¶é—´(ç§’)"] for result in comparison_results]

        fig_time.add_trace(go.Bar(
            x=algorithms,
            y=time_values,
            marker_color='#6A0572',
            text=[f'{x:.2f}s' for x in time_values],
            textposition='auto'
        ))

        fig_time.update_layout(
            title="è®­ç»ƒæ—¶é—´å¯¹æ¯”ï¼ˆç§’ï¼‰",
            xaxis_title="ç®—æ³•",
            yaxis_title="è®­ç»ƒæ—¶é—´ (ç§’)"
        )
        st.plotly_chart(fig_time, use_container_width=True)

    # ================== ç®—æ³•æ’åå’Œå»ºè®® ==================
    st.markdown("### ğŸ† ç®—æ³•æ€§èƒ½æ’å")

    # æŒ‰RÂ²æ’å
    ranked_by_r2 = sorted(comparison_results, key=lambda x: x['RÂ²'], reverse=True)
    # æŒ‰MAEæ’å
    ranked_by_mae = sorted(comparison_results, key=lambda x: x['MAE'])
    # æŒ‰è®­ç»ƒæ—¶é—´æ’å
    ranked_by_time = sorted(comparison_results, key=lambda x: x['è®­ç»ƒæ—¶é—´(ç§’)'])

    col1, col2, col3 = st.columns(3)

    with col1:
        st.markdown("**ğŸ¥‡ ç²¾åº¦æ’å (RÂ²)**")
        for i, result in enumerate(ranked_by_r2):
            medal = ["ğŸ¥‡", "ğŸ¥ˆ", "ğŸ¥‰"][i] if i < 3 else f"{i + 1}."
            st.write(f"{medal} {result['ç®—æ³•']}: {result['RÂ²']:.4f}")

    with col2:
        st.markdown("**ğŸ¯ è¯¯å·®æ’å (MAE)**")
        for i, result in enumerate(ranked_by_mae):
            medal = ["ğŸ¥‡", "ğŸ¥ˆ", "ğŸ¥‰"][i] if i < 3 else f"{i + 1}."
            st.write(f"{medal} {result['ç®—æ³•']}: {result['MAE']:.3f}")

    with col3:
        st.markdown("**âš¡ é€Ÿåº¦æ’å**")
        for i, result in enumerate(ranked_by_time):
            medal = ["ğŸ¥‡", "ğŸ¥ˆ", "ğŸ¥‰"][i] if i < 3 else f"{i + 1}."
            st.write(f"{medal} {result['ç®—æ³•']}: {result['è®­ç»ƒæ—¶é—´(ç§’)']:.2f}s")

    # æ€»ç»“å»ºè®®
    st.markdown("### ğŸ’¡ ç®—æ³•é€‰æ‹©å»ºè®®")
    best_model = ranked_by_r2[0]['ç®—æ³•']
    best_r2 = ranked_by_r2[0]['RÂ²']
    fastest_model = ranked_by_time[0]['ç®—æ³•']

    st.info(f"""
    **æ¨èç®—æ³•**: **{best_model}** (RÂ² = {best_r2:.4f})

    - ğŸ¯ **è¿½æ±‚æœ€é«˜ç²¾åº¦**: é€‰æ‹© **{best_model}**
    - âš¡ **æ³¨é‡è®­ç»ƒé€Ÿåº¦**: é€‰æ‹© **{fastest_model}**
    - âš–ï¸ **è¦æ±‚å¹³è¡¡æ€§**: å»ºè®®å°è¯• **{ranked_by_r2[1]['ç®—æ³•'] if len(ranked_by_r2) > 1 else best_model}**
    - ğŸ“Š **ç»¼åˆè€ƒè™‘**: æŸ¥çœ‹å„æŒ‡æ ‡é€‰æ‹©æœ€é€‚åˆä¸šåŠ¡åœºæ™¯çš„ç®—æ³•
    """)


# ===================== æ·±åº¦åˆ†æ =====================
def deep_analysis(df, datetime_col, target_column):
    st.subheader("ğŸ” æ·±åº¦åˆ†ææ¨¡å¼")

    st.info("""
    æ·±åº¦åˆ†ææ¨¡å¼æä¾›ï¼š
    - ç‰¹å¾å·¥ç¨‹å’Œé€‰æ‹©å»ºè®®
    - æ¨¡å‹å¯è§£é‡Šæ€§åˆ†æ
    - è¯¯å·®æ¨¡å¼è¯†åˆ«
    - é¢„æµ‹ä¸ç¡®å®šæ€§è¯„ä¼°
    - ä¸šåŠ¡æ´å¯Ÿæå–
    """)

    # ç‰¹å¾åˆ†æ
    st.subheader("ğŸ“ˆ ç‰¹å¾åˆ†æ")

    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    feature_candidates = [c for c in numeric_cols if c != target_column]

    # ç‰¹å¾ä¸ç›®æ ‡çš„ç›¸å…³æ€§
    if len(feature_candidates) > 0:
        correlations = {}
        for feature in feature_candidates:
            if feature in df.columns:
                corr = df[feature].corr(df[target_column])
                correlations[feature] = corr

        corr_df = pd.DataFrame({
            'ç‰¹å¾': list(correlations.keys()),
            'ç›¸å…³æ€§': list(correlations.values())
        }).sort_values('ç›¸å…³æ€§', key=abs, ascending=False)

        fig = px.bar(corr_df.head(15), x='ç›¸å…³æ€§', y='ç‰¹å¾', orientation='h',
                     title="ç‰¹å¾ä¸é£é€Ÿçš„ç›¸å…³æ€§æ’åº", color='ç›¸å…³æ€§',
                     color_continuous_scale='RdBu_r')
        st.plotly_chart(fig, use_container_width=True)

    # æ—¶é—´ç‰¹å¾åˆ†æ
    st.subheader("â° æ—¶é—´ç‰¹å¾åˆ†æ")

    col1, col2 = st.columns(2)

    with col1:
        # å°æ—¶åˆ†æ
        if 'hour' in df.columns:
            hourly_avg = df.groupby('hour')[target_column].agg(['mean', 'std']).reset_index()
            fig = go.Figure()
            fig.add_trace(go.Scatter(
                x=hourly_avg['hour'], y=hourly_avg['mean'],
                mode='lines+markers', name='å¹³å‡é£é€Ÿ',
                line=dict(color='blue', width=3)
            ))
            fig.add_trace(go.Scatter(
                x=hourly_avg['hour'], y=hourly_avg['mean'] + hourly_avg['std'],
                mode='lines', name='+1æ ‡å‡†å·®',
                line=dict(color='red', dash='dash', width=1)
            ))
            fig.add_trace(go.Scatter(
                x=hourly_avg['hour'], y=hourly_avg['mean'] - hourly_avg['std'],
                mode='lines', name='-1æ ‡å‡†å·®',
                line=dict(color='red', dash='dash', width=1),
                fill='tonexty'
            ))
            fig.update_layout(
                title="ä¸åŒå°æ—¶çš„å¹³å‡é£é€Ÿå˜åŒ–",
                xaxis_title="å°æ—¶",
                yaxis_title="å¹³å‡é£é€Ÿ (m/s)"
            )
            st.plotly_chart(fig, use_container_width=True)

    with col2:
        # æœˆä»½åˆ†æ
        if 'month' in df.columns:
            monthly_avg = df.groupby('month')[target_column].agg(['mean', 'std']).reset_index()
            fig = go.Figure()
            fig.add_trace(go.Bar(
                x=monthly_avg['month'], y=monthly_avg['mean'],
                name='å¹³å‡é£é€Ÿ',
                error_y=dict(type='data', array=monthly_avg['std'], visible=True)
            ))
            fig.update_layout(
                title="ä¸åŒæœˆä»½çš„å¹³å‡é£é€Ÿ",
                xaxis_title="æœˆä»½",
                yaxis_title="å¹³å‡é£é€Ÿ (m/s)"
            )
            st.plotly_chart(fig, use_container_width=True)

    # é£é€Ÿåˆ†å¸ƒåˆ†æ
    st.subheader("ğŸ“Š é£é€Ÿåˆ†å¸ƒç‰¹æ€§")

    col1, col2 = st.columns(2)

    with col1:
        # åˆ†å¸ƒç›´æ–¹å›¾
        fig = px.histogram(df, x=target_column, nbins=50,
                           title="é£é€Ÿåˆ†å¸ƒç›´æ–¹å›¾",
                           marginal="box")
        st.plotly_chart(fig, use_container_width=True)

    with col2:
        # ç»Ÿè®¡ç‰¹æ€§
        wind_speed = df[target_column]
        stats_data = {
            'æŒ‡æ ‡': ['å¹³å‡å€¼', 'ä¸­ä½æ•°', 'æ ‡å‡†å·®', 'ååº¦', 'å³°åº¦', 'å˜å¼‚ç³»æ•°'],
            'æ•°å€¼': [
                wind_speed.mean(),
                wind_speed.median(),
                wind_speed.std(),
                stats.skew(wind_speed.dropna()),
                stats.kurtosis(wind_speed.dropna()),
                wind_speed.std() / wind_speed.mean()
            ]
        }
        stats_df = pd.DataFrame(stats_data)
        stats_df['æ•°å€¼'] = stats_df['æ•°å€¼'].round(4)
        st.dataframe(stats_df, use_container_width=True)

    # é£é€Ÿç­‰çº§åˆ†æ
    st.subheader("ğŸŒªï¸ é£é€Ÿç­‰çº§åˆ†å¸ƒ")

    # å®šä¹‰é£é€Ÿç­‰çº§
    wind_bins = [0, 3, 6, 9, 12, 15, float('inf')]
    wind_labels = ['å¾®é£(0-3)', 'è½»é£(3-6)', 'ä¸­é£(6-9)', 'å¼ºé£(9-12)', 'å¤§é£(12-15)', 'æš´é£(15+)']

    df['wind_level'] = pd.cut(df[target_column], bins=wind_bins, labels=wind_labels)
    wind_level_count = df['wind_level'].value_counts().sort_index()

    col1, col2 = st.columns(2)

    with col1:
        fig = px.pie(values=wind_level_count.values, names=wind_level_count.index,
                     title="é£é€Ÿç­‰çº§åˆ†å¸ƒé¥¼å›¾")
        st.plotly_chart(fig, use_container_width=True)

    with col2:
        fig = px.bar(x=wind_level_count.index, y=wind_level_count.values,
                     title="é£é€Ÿç­‰çº§é¢‘æ¬¡åˆ†å¸ƒ")
        fig.update_layout(xaxis_title="é£é€Ÿç­‰çº§", yaxis_title="é¢‘æ¬¡")
        st.plotly_chart(fig, use_container_width=True)

    # æ¨èç‰¹å¾é›†
    st.subheader("ğŸ’¡ ç‰¹å¾å·¥ç¨‹å»ºè®®")

    # åŸºäºç›¸å…³æ€§çš„ç‰¹å¾æ¨è
    if len(feature_candidates) > 0:
        high_corr_features = corr_df[abs(corr_df['ç›¸å…³æ€§']) > 0.1]['ç‰¹å¾'].tolist()

        st.write("**æ¨èç‰¹å¾é›†ï¼ˆåŸºäºç›¸å…³æ€§ï¼‰:**")
        for feature in high_corr_features[:10]:
            corr_val = corr_df[corr_df['ç‰¹å¾'] == feature]['ç›¸å…³æ€§'].values[0]
            st.write(f"- `{feature}` (ç›¸å…³æ€§: {corr_val:.3f})")

        if len(high_corr_features) == 0:
            st.info("æœªå‘ç°å¼ºç›¸å…³ç‰¹å¾ï¼Œå»ºè®®ä½¿ç”¨æ‰€æœ‰å¯ç”¨ç‰¹å¾æˆ–è¿›è¡Œç‰¹å¾å·¥ç¨‹")


# ===================== è¾…åŠ©å‡½æ•° =====================
def calculate_metrics(y_true, y_pred, training_time):
    """è®¡ç®—æ¨¡å‹è¯„ä¼°æŒ‡æ ‡"""
    return {
        'true': np.array(y_true),
        'pred': np.array(y_pred),
        'mae': mean_absolute_error(y_true, y_pred),
        'rmse': np.sqrt(mean_squared_error(y_true, y_pred)),
        'r2': r2_score(y_true, y_pred),
        'training_time': training_time
    }


def calculate_permutation_importance(model, X_test, y_test, feature_names, n_repeats=5):
    """è®¡ç®—ç½®æ¢é‡è¦æ€§"""
    try:
        # ä½¿ç”¨sklearnçš„permutation_importance
        from sklearn.inspection import permutation_importance

        result = permutation_importance(
            model, X_test, y_test,
            n_repeats=n_repeats,
            random_state=42,
            n_jobs=-1
        )

        # åˆ›å»ºé‡è¦æ€§DataFrame
        importance_df = pd.DataFrame({
            'feature': feature_names,
            'importance': result.importances_mean,
            'std': result.importances_std
        }).sort_values('importance', ascending=False)

        return importance_df

    except Exception as e:
        st.warning(f"ç½®æ¢é‡è¦æ€§è®¡ç®—å¤±è´¥: {str(e)}")
        return None


# ===================== ç»“æœæ˜¾ç¤ºå‡½æ•° =====================
def display_single_model_results(results, feature_importance, permutation_importance_result,
                                 model_name, y_true, y_pred, cv_scores, X_test, model, history=None):
    st.subheader(f"ğŸ“Š {model_name} æ¨¡å‹æ€§èƒ½")

    # æŒ‡æ ‡å¡ç‰‡
    col1, col2, col3, col4, col5 = st.columns(5)
    col1.metric("MAE", f"{results['mae']:.3f}")
    col2.metric("RMSE", f"{results['rmse']:.3f}")
    col3.metric("RÂ²", f"{results['r2']:.4f}")
    col4.metric("è®­ç»ƒæ—¶é—´", f"{results['training_time']:.2f}s")

    if model_name not in ["LSTM", "GRU"]:
        col5.metric("CV RÂ²", f"{results['cv_mean']:.4f}")
    else:
        col5.metric("éªŒè¯æŸå¤±", f"{history.history['val_loss'][-1]:.4f}" if history else "N/A")

    # å¯è§†åŒ–æ ‡ç­¾é¡µ
    tab_names = ["é¢„æµ‹æ•ˆæœ", "æ®‹å·®åˆ†æ", "ç‰¹å¾é‡è¦æ€§", "äº¤å‰éªŒè¯", "è¯¯å·®åˆ†æ", "æ¨¡å‹è¯Šæ–­"]
    if model_name in ["LSTM", "GRU"]:
        tab_names.insert(3, "è®­ç»ƒè¿‡ç¨‹")

    tabs = st.tabs(tab_names)

    with tabs[0]:
        # é¢„æµ‹ vs çœŸå®å€¼
        fig = go.Figure()
        fig.add_trace(go.Scatter(
            x=y_true, y=y_pred, mode='markers',
            marker=dict(color='royalblue', opacity=0.6),
            name='é¢„æµ‹ç‚¹'
        ))
        min_val, max_val = float(np.min(y_true)), float(np.max(y_true))
        fig.add_trace(go.Scatter(
            x=[min_val, max_val], y=[min_val, max_val],
            mode='lines', line=dict(dash='dash', color='red'),
            name='ç†æƒ³æ‹Ÿåˆ'
        ))
        fig.update_layout(
            title="é¢„æµ‹å€¼ vs çœŸå®å€¼",
            xaxis_title="çœŸå®é£é€Ÿ (m/s)",
            yaxis_title="é¢„æµ‹é£é€Ÿ (m/s)"
        )
        st.plotly_chart(fig, use_container_width=True)

        # é¢„æµ‹æ—¶é—´åºåˆ—ï¼ˆå¦‚æœæ•°æ®æœ‰åºï¼‰
        sample_size = min(200, len(y_true))
        if len(y_true) > sample_size:
            try:
                indices = np.random.choice(len(y_true), size=sample_size, replace=False)
                if hasattr(y_true, 'iloc'):
                    y_true_sample = y_true.iloc[indices]
                else:
                    y_true_sample = y_true[indices]
                y_pred_sample = y_pred[indices]

                fig_ts = go.Figure()
                fig_ts.add_trace(go.Scatter(
                    y=y_true_sample,
                    mode='lines+markers', name='çœŸå®å€¼'
                ))
                fig_ts.add_trace(go.Scatter(
                    y=y_pred_sample, mode='lines+markers', name='é¢„æµ‹å€¼'
                ))
                fig_ts.update_layout(title="é¢„æµ‹å€¼æ—¶é—´åºåˆ—å¯¹æ¯”ï¼ˆé‡‡æ ·ï¼‰")
                st.plotly_chart(fig_ts, use_container_width=True)
            except Exception as e:
                st.warning(f"æ—¶é—´åºåˆ—é‡‡æ ·å¤±è´¥: {str(e)}")

    with tabs[1]:
        # æ®‹å·®åˆ†æ
        residuals = y_true - y_pred

        fig = sp.make_subplots(
            rows=2, cols=2,
            subplot_titles=('æ®‹å·®åˆ†å¸ƒ', 'æ®‹å·® vs é¢„æµ‹å€¼', 'æ®‹å·®QQå›¾', 'æ®‹å·®è‡ªç›¸å…³'),
            specs=[[{"secondary_y": False}, {"secondary_y": False}],
                   [{"secondary_y": False}, {"secondary_y": False}]]
        )

        # æ®‹å·®åˆ†å¸ƒ
        fig.add_trace(go.Histogram(x=residuals, nbinsx=50, name='æ®‹å·®åˆ†å¸ƒ'), row=1, col=1)

        # æ®‹å·® vs é¢„æµ‹å€¼
        fig.add_trace(go.Scatter(x=y_pred, y=residuals, mode='markers', name='æ®‹å·®'), row=1, col=2)
        fig.add_hline(y=0, line_dash='dash', line_color='red', row=1, col=2)

        # QQå›¾
        theoretical_quantiles = stats.probplot(residuals, dist="norm")
        fig.add_trace(go.Scatter(
            x=theoretical_quantiles[0][0], y=theoretical_quantiles[0][1],
            mode='markers', name='QQå›¾'
        ), row=2, col=1)
        fig.add_trace(go.Scatter(
            x=[min(theoretical_quantiles[0][0]), max(theoretical_quantiles[0][0])],
            y=[min(theoretical_quantiles[0][0]), max(theoretical_quantiles[0][0])],
            mode='lines', name='å‚è€ƒçº¿', line=dict(dash='dash')
        ), row=2, col=1)

        # æ®‹å·®è‡ªç›¸å…³
        autocorr = [1.0] + [np.corrcoef(residuals[:-i], residuals[i:])[0, 1] for i in range(1, 21)]
        fig.add_trace(go.Scatter(
            x=list(range(len(autocorr))), y=autocorr,
            mode='lines+markers', name='è‡ªç›¸å…³'
        ), row=2, col=2)
        fig.add_hline(y=0, line_dash='dash', line_color='gray', row=2, col=2)

        fig.update_layout(height=600, showlegend=False)
        st.plotly_chart(fig, use_container_width=True)

    with tabs[2]:
        # ç‰¹å¾é‡è¦æ€§
        col1, col2 = st.columns(2)

        with col1:
            if feature_importance is not None:
                fig = px.bar(feature_importance.head(10), x='importance', y='feature',
                             title="Top 10 ç‰¹å¾é‡è¦æ€§ï¼ˆå†…ç½®ï¼‰")
                st.plotly_chart(fig, use_container_width=True)
            else:
                st.info("è¯¥æ¨¡å‹ä¸æ”¯æŒå†…ç½®ç‰¹å¾é‡è¦æ€§åˆ†æ")

        with col2:
            if permutation_importance_result is not None:
                fig = px.bar(permutation_importance_result.head(10), x='importance', y='feature',
                             title="Top 10 ç½®æ¢é‡è¦æ€§")
                st.plotly_chart(fig, use_container_width=True)
            else:
                st.info("æœªè®¡ç®—ç½®æ¢é‡è¦æ€§")

    # æ·±åº¦å­¦ä¹ è®­ç»ƒè¿‡ç¨‹å¯è§†åŒ–
    if model_name in ["LSTM", "GRU"] and history:
        with tabs[3]:
            st.subheader("ğŸ“ˆ è®­ç»ƒè¿‡ç¨‹ç›‘æ§")

            fig_loss = go.Figure()
            fig_loss.add_trace(go.Scatter(
                y=history.history['loss'],
                mode='lines',
                name='è®­ç»ƒæŸå¤±'
            ))
            fig_loss.add_trace(go.Scatter(
                y=history.history['val_loss'],
                mode='lines',
                name='éªŒè¯æŸå¤±'
            ))
            fig_loss.update_layout(
                title="è®­ç»ƒå’ŒéªŒè¯æŸå¤±æ›²çº¿",
                xaxis_title="è®­ç»ƒè½®æ•°",
                yaxis_title="æŸå¤±å€¼"
            )
            st.plotly_chart(fig_loss, use_container_width=True)

            # æ˜¾ç¤ºæ¨¡å‹ç»“æ„ä¿¡æ¯
            st.subheader("ğŸ› ï¸ æ¨¡å‹ç»“æ„ä¿¡æ¯")
            model_summary = []
            model.summary(print_fn=lambda x: model_summary.append(x))
            st.text_area("æ¨¡å‹ç»“æ„", "\n".join(model_summary), height=200)

    # è°ƒæ•´åç»­æ ‡ç­¾é¡µçš„ç´¢å¼•
    offset = 1 if model_name in ["LSTM", "GRU"] else 0

    with tabs[3 + offset]:
        # äº¤å‰éªŒè¯ç»“æœ
        if model_name not in ["LSTM", "GRU"]:
            fig = go.Figure()
            fig.add_trace(go.Bar(
                x=list(range(1, len(cv_scores) + 1)),
                y=cv_scores,
                marker_color='lightgreen',
                name='æ¯æŠ˜RÂ²'
            ))
            fig.add_hline(y=results['cv_mean'], line_dash='dash', line_color='red',
                          annotation_text=f'å¹³å‡ RÂ²: {results["cv_mean"]:.4f}')
            fig.update_layout(
                title="äº¤å‰éªŒè¯ç»“æœ",
                xaxis_title="æŠ˜æ•°",
                yaxis_title="RÂ² åˆ†æ•°"
            )
            st.plotly_chart(fig, use_container_width=True)

            st.write(f"äº¤å‰éªŒè¯ç¨³å®šæ€§: {results['cv_std']:.4f} (æ ‡å‡†å·®)")
        else:
            st.info("æ·±åº¦å­¦ä¹ æ¨¡å‹ä½¿ç”¨éªŒè¯é›†è¿›è¡Œæ€§èƒ½è¯„ä¼°")

    with tabs[4 + offset]:
        # è¯¯å·®åˆ†æ
        absolute_errors = np.abs(y_true - y_pred)
        relative_errors = np.abs((y_true - y_pred) / np.where(y_true == 0, 1e-10, y_true))

        col1, col2 = st.columns(2)
        with col1:
            st.metric("æœ€å¤§ç»å¯¹è¯¯å·®", f"{np.max(absolute_errors):.3f}")
            st.metric("è¯¯å·®æ ‡å‡†å·®", f"{np.std(absolute_errors):.3f}")
            st.metric("å¹³å‡ç›¸å¯¹è¯¯å·®", f"{np.mean(relative_errors) * 100:.1f}%")
        with col2:
            st.metric("è¯¯å·® < 0.5 m/s", f"{np.mean(absolute_errors < 0.5) * 100:.1f}%")
            st.metric("è¯¯å·® < 1.0 m/s", f"{np.mean(absolute_errors < 1.0) * 100:.1f}%")
            st.metric("è¯¯å·® < 2.0 m/s", f"{np.mean(absolute_errors < 2.0) * 100:.1f}%")

        # è¯¯å·®åˆ†å¸ƒ
        fig = px.histogram(x=absolute_errors, nbins=50, title="ç»å¯¹è¯¯å·®åˆ†å¸ƒ")
        st.plotly_chart(fig, use_container_width=True)

    with tabs[5 + offset]:
        # æ¨¡å‹è¯Šæ–­
        st.subheader("æ¨¡å‹è¯Šæ–­ä¿¡æ¯")

        if model_name not in ["LSTM", "GRU"]:
            # å­¦ä¹ æ›²çº¿åˆ†æï¼ˆç®€åŒ–ç‰ˆï¼‰
            train_sizes = [0.1, 0.3, 0.5, 0.7, 0.9]
            train_scores = []

            for size in train_sizes:
                n_samples = int(len(X_test) * size)
                if n_samples > 0:
                    X_subset = X_test[:n_samples]
                    y_subset = y_true[:n_samples]
                    pred_subset = model.predict(X_subset)
                    train_scores.append(r2_score(y_subset, pred_subset))

            if train_scores:
                fig = go.Figure()
                fig.add_trace(go.Scatter(
                    x=[size for size in train_sizes[:len(train_scores)]],
                    y=train_scores,
                    mode='lines+markers',
                    name='æµ‹è¯•é›†RÂ²'
                ))
                fig.update_layout(
                    title="æ¨¡å‹æ€§èƒ½éšæ•°æ®é‡å˜åŒ–",
                    xaxis_title="æ•°æ®æ¯”ä¾‹",
                    yaxis_title="RÂ²åˆ†æ•°"
                )
                st.plotly_chart(fig, use_container_width=True)
            else:
                st.info("æ— æ³•è®¡ç®—å­¦ä¹ æ›²çº¿")

            # æ¨¡å‹ç¨³å®šæ€§åˆ†æ
            st.write("**æ¨¡å‹ç¨³å®šæ€§åˆ†æ**")
            col1, col2 = st.columns(2)
            with col1:
                st.metric("äº¤å‰éªŒè¯æ ‡å‡†å·®", f"{results['cv_std']:.4f}")
            with col2:
                stability = "é«˜" if results['cv_std'] < 0.05 else "ä¸­ç­‰" if results['cv_std'] < 0.1 else "ä½"
                st.metric("ç¨³å®šæ€§è¯„çº§", stability)
        else:
            st.info("æ·±åº¦å­¦ä¹ æ¨¡å‹è¯Šæ–­ä¿¡æ¯å·²åœ¨è®­ç»ƒè¿‡ç¨‹æ ‡ç­¾é¡µä¸­æ˜¾ç¤º")